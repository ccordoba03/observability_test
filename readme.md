# Proyecto: Observabilidad en AWS EKS.

**√öltima actualizaci√≥n:** 12/01/2025
Se reinicializa el proyecto bajo un nuevo eks  cluster name  eks-observability-v2
Se optimizan nodos para evitar sobre costo en cuenta aws personal
Se soluciona problema de compatibilidad en grafanna alloy y se expone por medio de aws prometeus hacia grafana UI por medio de ingress.


---

## üåê URLs de Acceso

### Aplicaci√≥n Hello World
**URL:** http://k8s-appdemo-hellowor-6ac75a7fca-1457236690.us-east-1.elb.amazonaws.com

### Grafana UI
**URL:** http://k8s-observab-grafana-536238b7ca-1103291513.us-east-1.elb.amazonaws.com

**Credenciales:**
- Usuario: `admin`
- Password: `admin123`

### Amazon Managed Prometheus
**Workspace ID:** ws-1c2bc642-d761-4c63-a58e-e12da54d36f1  
**Regi√≥n:** us-east-1  
**Endpoint:** https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-1c2bc642-d761-4c63-a58e-e12da54d36f1/

-

### 2. Configurar kubeconfig
```bash
aws eks update-kubeconfig --name eks-observability-v2 --region us-east-1
```

### 3. Desplegar Grafana Alloy
```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
helm install grafana-alloy grafana/alloy -n observability -f alloy-final-values.yaml
```

### 4. Desplegar Grafana
```bash
helm repo add grafana https://grafana.github.io/charts
helm install grafana grafana/grafana -n observability -f grafana-values.yaml
kubectl apply -f grafana-ingress.yaml
```

### 5. Verificar Despliegue
```bash
# Verificar todos los pods
kubectl get pods -A

# Verificar Ingress
kubectl get ingress -A

# Verificar logs de Alloy
kubectl logs -n observability -l app.kubernetes.io/name=alloy -c alloy --tail=50
```
Tomado de https://grafana.com/docs/alloy/latest/configure/kubernetes/
---

## üìä Informaci√≥n del Cluster

**Nombre:** eks-observability-v2  
**Versi√≥n:** 1.29  
**Regi√≥n:** us-east-1  
**VPC CIDR:** 10.0.0.0/16  
**Account ID:** 905418343592  
**OIDC Provider:** FD2B7A8AB4911A4FF83F2933038345AB

**Nodos:**
- Bootstrap: 1x t3.medium (managed node group)
- Karpenter: Din√°mico (t3.medium, t3.large)

**Componentes Principales:**
- ‚úÖ AWS Load Balancer Controller (v2.17.1)
- ‚úÖ Karpenter (v0.37.0)
- ‚úÖ Grafana Alloy (DaemonSet)
- ‚úÖ Grafana (Deployment)
- ‚úÖ Amazon Managed Prometheus
- ‚úÖ External Secrets Operator

---

## üìù Notas Importantes

1. **Persistencia de Grafana:** Deshabilitada (sin EBS CSI driver). Los dashboards/configuraciones se pierden al reiniciar el pod.
2. **Costos:** Recursos desplegados generan costos en AWS (EKS, EC2, NAT Gateway, ALB, AMP).
3. **Seguridad:** Endpoints p√∫blicos restringidos a IP espec√≠fica (181.53.12.236/32) en `allowed_cidrs`.
4. **Retenci√≥n AMP:** 150 d√≠as por defecto.
5. **Karpenter:** Consolida nodos autom√°ticamente si utilizaci√≥n < 50%.

---

## üîí Seguridad y RBAC

### Roles configurados:
- **cluster-admin:** Acceso completo al cluster
- **developer:** Acceso limitado a namespace `developer-ns` (view, create pods/deployments)

### IAM Roles (IRSA):
- `KarpenterController-*` ‚Äî Karpenter node provisioning
- `ALBControllerRole-*` ‚Äî ALB/NLB management
- `GrafanaAlloyRole-*` ‚Äî AMP remote_write
- `AppDemoRole-*` ‚Äî Secrets Manager access
- `ExternalSecretsRole-*` ‚Äî Secrets Manager sync

---

## üìö Referencias tomadas para realizar prueba

- [Amazon EKS Best Practices](https://aws.github.io/aws-eks-best-practices/)
- [Karpenter Documentation](https://karpenter.sh/)
- [Grafana Alloy Documentation](https://grafana.com/docs/alloy/)
- [Amazon Managed Prometheus](https://aws.amazon.com/prometheus/)
- [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/)

Gu√≠a t√©cnica detallada de despliegue de infraestructura, cluster EKS y componentes de observabilidad en AWS, automatizado con Terraform.

El siguiente documento expone la consecucion de hitos y su integracion tecnica hacia aws (eks,aws console) y como se despliega desde terraform IAC.

 üìã Vista General del Cluster 

A continuacion se describen de manera general  los componentes mas importantes del data plane :

### Namespaces esperados en el cluster:
PS C:\Users\cpuo\Observability_test> kubectl get namespaces
NAME                      STATUS   AGE
app-demo                  Active   5h
default                   Active   34h
developer-ns              Active   5h
external-secrets-system   Active   4h47m
karpenter                 Active   33h
kube-node-lease           Active   34h
kube-public               Active   34h
kube-system               Active   34h
observability             Active   5h
PS C:\Users\cpuo\Observability_test> 

 Descripcion 
1. **`kube-system`** ‚Äî Componentes core de Kubernetes (DNS, proxy de red).
2. **`kube-public`** ‚Äî Recursos p√∫blicos de lectura.
3. **`default`** ‚Äî Namespace por defecto.
4. **`observability`** ‚Äî Grafana Alloy y componentes de monitoreo.
5. **`karpenter`** ‚Äî Controller de Karpenter para node provisioning.
6. **`kube-node-lease`** ‚Äî Heartbeats de nodos.
7. **`app-demo`** ‚Äî Aplicaci√≥n Hello World y su ESO/CSI.
8. **`external-secrets-system`** ‚Äî ESO controller.
9. **`istio-system`** (opcional) ‚Äî Control plane y gateways de Istio.



### Pods esperados por namespace (resumen):

| Namespace | Pod/Componente | Tipo | Cantidad | Descripci√≥n |
|-----------|----------------|------|----------|-------------|
| `kube-system` | coredns | Deployment | 2 | Resoluci√≥n DNS |
| `kube-system` | aws-node | DaemonSet | N* | Plugin CNI de AWS (un pod por nodo) |
| `kube-system` | kube-proxy | DaemonSet | N* | Proxy de red (un pod por nodo) |
| `observability` | grafana-alloy | DaemonSet | N* | Agent de observabilidad (un pod por nodo) |
| `observability` | kube-state-metrics | Deployment | 1 | Exportador de estado de Kubernetes |
| `observability` | prometheus-node-exporter | DaemonSet | N* | M√©tricas del nodo (CPU, memoria, disco) |
| `karpenter` | karpenter | Deployment | 1 | Controller de provisioning de nodos |
| `app-demo` | hello-world | Deployment | 1-3 | Aplicaci√≥n demo (stateless, escalable) |
| `app-demo` | external-secrets | Deployment | 1 | ESO controller (opcional si no se usa CSI) |
| `external-secrets-system` | external-secrets | Deployment | 1 | ESO controller (instalaci√≥n dedicada) |
| `istio-system` | istiod | Deployment | 1 | Control plane de Istio (opcional) |
| `istio-system` | istio-ingressgateway | Deployment | 1 | Ingress gateway de Istio (opcional) |

*\*N = n√∫mero de nodos en el cluster.*

**Total esperado: 12-20+ pods** (dependiendo de r√©plicas y si Istio est√° habilitado).

### Nodos a nivel cluster (resumen):

PS C:\Users\cpuo\Observability_test> kubectl get nodes     
NAME                         STATUS   ROLES    AGE   VERSION
ip-10-0-1-163.ec2.internal   Ready    <none>   29h   v1.29.15-eks-ecaa3a6
ip-10-0-2-232.ec2.internal   Ready    <none>   30h   v1.29.15-eks-ecaa3a6

Cada nodo ejecuta:
- `aws-node` (plugin CNI).
- `kube-proxy`.
- `grafana-alloy` (DaemonSet).
- `prometheus-node-exporter` (opcional).
- Pods de aplicaci√≥n (`hello-world`, controllers, etc.).

---

---

## HITO 1: Infraestructura Base (VPC + EKS)


Crear la infraestructura base: VPC multi-AZ, subnets p√∫blicas/privadas, NAT Gateway e inicializar cluster EKS administrado con endpoint API restringido,configuracion de dos tipos de roles para la administracion general del cluster

### Integraci√≥n con AWS
- **VPC:** Red aislada 10.0.0.0/16 con 3 AZs.
  - Subnets p√∫blicas (10.0.101.0/24, 10.0.102.0/24, 10.0.103.0/24) para ALB.
  - Subnets privadas (10.0.1.0/24, 10.0.2.0/24, 10.0.3.0/24) para nodos y pods.
  - NAT Gateway en cada AZ para egress de pods.
  - Internet Gateway para acceso p√∫blico.
- **Security Groups:** Controlar tr√°fico entre subnets, nodos y ALB.
- **IAM Roles:** Rol para nodos de EKS (asume rol de EC2), OIDC provider para IRSA.
- **EKS:** Cl√∫ster administrado (control plane gestionado por AWS, API server privado o restringido).

### Integraci√≥n con EKS
- Cluster EKS se crea con endpoint `private_access = true` y `public_access_cidrs` limitados.
- Se configura `aws-auth` ConfigMap para mapear usuarios/roles IAM a RBAC de Kubernetes.
- Se crean RoleBindings iniciales: `cluster-admin` para usuarios administrativos, `view-only` para developers.

### Despliegue con Terraform
- **M√≥dulo:** `terraform/modules/network` (o en `main.tf` si no hay m√≥dulos).
- **Ubicaci√≥n:** `terraform/main.tf` (recursos VPC, subnets, route tables, NAT).
- **Recursos clave:**
  - `aws_vpc` ‚Äî VPC principal.
  - `aws_subnet` ‚Äî Subnets p√∫blicas y privadas (6 totales, 2-3 por AZ).
  - `aws_nat_gateway` ‚Äî 1 por AZ (3 totales) para egress.
  - `aws_internet_gateway` ‚Äî Acceso p√∫blico.
  - `aws_eks_cluster` ‚Äî Cl√∫ster EKS.
  - `aws_iam_role` ‚Äî Rol para nodos y control plane.
  - `aws_iam_openid_connect_provider` ‚Äî OIDC provider para IRSA.

### Componentes en el Cluster (resultado del Hito 1)
**Namespaces:** `kube-system`, `kube-public`, `default`.

**Pods auto-creados por EKS:**
- `coredns` (2 replicas en `kube-system`) ‚Äî Resoluci√≥n DNS.
- `aws-node` DaemonSet (N pods en `kube-system`) ‚Äî Plugin CNI de AWS.
- `kube-proxy` DaemonSet (N pods en `kube-system`) ‚Äî Proxy de red.

**Nodos:** 2-3 nodos iniciales (EC2) en subnets privadas.

**Recursos de RBAC:**
- `aws-auth` ConfigMap con mapeo de usuarios/roles IAM.
- ClusterRoles y ClusterRoleBindings para admin y developer.

---

## HITO 2: Gesti√≥n de Nodos (Karpenter)

### Objetivo
Reemplazar el node autoscaler tradicional (Cluster Autoscaler) con Karpenter, que provisiona nodos din√°micamente seg√∫n demanda de pods con pol√≠ticas declarativas.

### Integraci√≥n con AWS
- **EC2 API:** Karpenter llama a `ec2:RunInstances`, `ec2:TerminateInstances`, `ec2:Describe*` para crear/destruir instancias.
- **IAM Role (IRSA):** ServiceAccount `karpenter` anotado con rol que tiene permisos EC2 m√≠nimos.
- **Subnets y Security Groups:** Karpenter lanza nodos en subnets privadas etiquetadas con `karpenter.sh/discovery: <cluster-name>`.
- **Tags:** Karpenter etiqueta instancias con `kubernetes.io/cluster/<name>: owned` para que EKS las reconozca.

### Integraci√≥n con EKS
- **Namespace:** `karpenter` (creado por Helm).
- **ServiceAccount:** `karpenter` anotado con `eks.amazonaws.com/role-arn: arn:aws:iam::...role/KarpenterNodeRole`.
- **CRD:** `Provisioner` ‚Äî declarativo, configura tipos de instancia, subnets, consolidaci√≥n y TTL.
- **Pods:**
  - `karpenter-controller` (Deployment, 1 replica) ‚Äî Reconciler que monitorea pod requests y provisiona nodos.
  - `karpenter-webhook` (Deployment, 1 replica) ‚Äî Webhook para validar manifiestos.

### Despliegue con Terraform
- **M√≥dulo:** `terraform/modules/karpenter` (o integrado en `main.tf`).
- **Ubicaci√≥n:** `terraform/main.tf` o `terraform/karpenter.tf`.
- **Recursos clave:**
  - `aws_iam_role` ‚Äî Rol KarpenterNodeRole (asume role desde EKS OIDC).
  - `aws_iam_role_policy` ‚Äî Permisos EC2 para Karpenter.
  - `helm_release` ‚Äî Chart de Karpenter (repo `oci://public.ecr.aws/karpenter`).
  - `kubernetes_namespace` ‚Äî Namespace `karpenter`.
  - `kubernetes_manifest` ‚Äî Provisioner CRD.

### Componentes en el Cluster (resultado del Hito 2)
**Namespace:** `karpenter`.

**Pods:**
- `karpenter-controller` (Deployment, 1 replica).
- `karpenter-webhook` (Deployment, 1 replica).

PS C:\Users\cpuo\Observability_test\terraform> kubectl get pods -n karpenter
NAME                         READY   STATUS    RESTARTS   AGE
karpenter-675bc46c6d-kj5n9   1/1     Running   0          3m52s
karpenter-675bc46c6d-vmrnm   1/1     Running   0          3m52s

**CRDs:**
- `Provisioner` ‚Äî Pol√≠tica de node provisioning.
- `AWSNodeTemplate` ‚Äî Template de nodos EC2.

**Nodos din√°micos:** Karpenter agrega nodos seg√∫n demanda; ejemplos:
- Si se solicita 1 GiB RAM, Karpenter lanza 1 nodo `t3.medium`.
- Si hay menos de 10% utilizaci√≥n, Karpenter consolida/termina nodos.

**Total de pods Karpenter: 2 (controller + webhook).**

---

## HITO 3: Observabilidad (Grafana Alloy + Amazon Managed Prometheus)

### Objetivo
Implementar stack completo de observabilidad con Grafana Alloy (recolecci√≥n), Amazon Managed Prometheus (almacenamiento) y Grafana (visualizaci√≥n), permitiendo monitorear el cluster EKS en tiempo real.

### ¬øPor qu√© Amazon Managed Prometheus (AMP)?

**Ventajas de AMP vs Prometheus auto-gestionado:**

1. **Sin Overhead Operacional**
   - No requiere gestionar servidores, storage, backups ni HA
   - AWS maneja escalado autom√°tico, durabilidad y disponibilidad
   - Sin preocupaciones por dimensionamiento de disco o retenci√≥n

2. **Integraci√≥n Nativa con AWS**
   - Autenticaci√≥n SigV4 (sin gesti√≥n de tokens o passwords)
   - IRSA (IAM Roles for Service Accounts) - permisos granulares por pod
   - Integraci√≥n con CloudWatch, X-Ray y otros servicios AWS

3. **Costo-Eficiencia**
   - Pago por uso (ingesta + almacenamiento + queries)
   - No requiere instancias EC2 dedicadas 24/7 para Prometheus
   - Retenci√≥n hasta 150 d√≠as sin gesti√≥n de storage

4. **Escalabilidad**
   - Soporta millones de m√©tricas activas sin tunning
   - Query performance optimizado por AWS
   - Compatible con PromQL est√°ndar

5. **Seguridad y Compliance**
   - Cifrado en tr√°nsito y reposo por defecto
   - VPC endpoints para tr√°fico privado (opcional)
   - AWS CloudTrail para auditor√≠a de accesos

### Integraci√≥n con AWS
- **Amazon Managed Prometheus (AMP):** Workspace dedicado para el cluster.
  - **Workspace ID:** `ws-1c2bc642-d761-4c63-a58e-e12da54d36f1`
  - **Endpoint:** `https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-1c2bc642-d761-4c63-a58e-e12da54d36f1/`
  - **Remote Write:** `https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-1c2bc642-d761-4c63-a58e-e12da54d36f1/api/v1/remote_write`
  - Autenticaci√≥n: SigV4 (firma con credenciales AWS temporales).
  - Estado: `ACTIVE`
  - Regi√≥n: `us-east-1`
  
- **IAM Role (IRSA):** Rol espec√≠fico para Alloy con permiso `aps:RemoteWrite` al workspace AMP.
  - **Rol:** `GrafanaAlloyRole-eks-observability-v2`
  - **Policy:** `AMPWritePolicy` (inline) con permisos `aps:RemoteWrite`
  - **Trust Policy:** Conf√≠a en OIDC provider del cluster con condici√≥n `system:serviceaccount:observability:grafana-alloy`

- **IAM Role para Grafana:** Rol para consultar m√©tricas de AMP (lectura).
  - **Autenticaci√≥n:** SigV4 autom√°tica v√≠a AWS SDK en datasource

### Integraci√≥n con EKS
- **Namespace:** `observability` (creado por Terraform/Helm).

#### Grafana Alloy (Recolecci√≥n de M√©tricas)

- **ServiceAccount:** `grafana-alloy` anotado con `eks.amazonaws.com/role-arn` para IRSA.
- **DaemonSet:** `grafana-alloy` ‚Äî 1 pod por nodo, recolecta m√©tricas locales.
  - **Imagen:** `grafana/alloy:latest`
  - **Configuraci√≥n:** Scraping de kubelet, cAdvisor, kube-apiserver y pods anotados
  - **Remote Write:** Env√≠a m√©tricas a AMP con SigV4
  - **Clustering:** Habilitado para evitar duplicados en scraping de targets centrales

**Configuraci√≥n de Alloy (alloy-final-values.yaml):**
- **Scrape Jobs:**
  1. `kubelet` ‚Äî M√©tricas de runtime de contenedores
  2. `cadvisor` ‚Äî M√©tricas de uso de CPU/memoria/red por contenedor
  3. `kube_apiserver` ‚Äî M√©tricas del API server (request rate, latency)
  4. `kubernetes_pods` ‚Äî Pods con annotation `prometheus.io/scrape=true`

- **Discovery:** `discovery.kubernetes` para auto-descubrimiento de nodos/endpoints/pods
- **Authentication:** SigV4 con regi√≥n `us-east-1` en remote_write
- **Queue Config:** `max_shards=200`, `capacity=10000` para buffering

#### Grafana (Visualizaci√≥n)

- **Deployment:** `grafana` (1 replica) ‚Äî UI web para consultar y visualizar m√©tricas
  - **Imagen:** `grafana/grafana:latest`
  - **Credenciales:** 
    - Usuario: `admin`
    - Password: `admin123`
  - **Datasource:** Amazon Managed Prometheus pre-configurado con SigV4
  - **Persistencia:** Deshabilitada (sin EBS CSI driver instalado)

- **Service:** ClusterIP (interno)
- **Ingress:** ALB Ingress para acceso p√∫blico
  - **URL:** http://k8s-observab-grafana-536238b7ca-1103291513.us-east-1.elb.amazonaws.com
  - **Health Check:** `/api/health`
  - **Esquema:** internet-facing
  - **Target Type:** IP (apunta directamente a pods)

### Despliegue con Terraform
- **M√≥dulo:** `terraform/modules/observability` (o integrado en `main.tf`).
- **Ubicaci√≥n:** `terraform/main.tf` o `terraform/observability.tf`.
- **Recursos clave:**
  - `module.prometheus` ‚Äî Terraform module para crear AMP workspace
  - `data.aws_prometheus_workspace` ‚Äî Data source para obtener detalles del workspace
  - `aws_iam_role.grafana_alloy` ‚Äî Rol IRSA para Alloy
  - `aws_iam_role_policy.amp_write` ‚Äî Policy inline con `aps:RemoteWrite`
  - `kubernetes_service_account.grafana_alloy` ‚Äî ServiceAccount anotado con role ARN
  - `kubernetes_namespace.observability` ‚Äî Namespace dedicado

**Nota:** Alloy y Grafana se despliegan manualmente via Helm debido a limitaciones del provider Kubernetes con configuraci√≥n compleja:

```bash
# Despliegue de Grafana Alloy
helm install grafana-alloy grafana/alloy -n observability \
  -f terraform/alloy-final-values.yaml

# Despliegue de Grafana
helm install grafana grafana/grafana -n observability \
  -f terraform/grafana-values.yaml

# Aplicar Ingress de Grafana
kubectl apply -f terraform/grafana-ingress.yaml
```

### Componentes en el Cluster (resultado del Hito 3)
**Namespace:** `observability`.

**Pods:**
```
NAME                       READY   STATUS    RESTARTS   AGE
grafana-alloy-2z7rf        2/2     Running   0          4h
grafana-alloy-xxxxx        2/2     Running   0          4h
grafana-6945894fdd-qhh9d   1/1     Running   0          15m
```

- **grafana-alloy** DaemonSet (N replicas, 1 por nodo) ‚Äî Recolector de m√©tricas
  - Container 1: `alloy` ‚Äî Agente principal
  - Container 2: `config-reloader` ‚Äî Recarga configuraci√≥n autom√°tica
- **grafana** Deployment (1 replica) ‚Äî UI de visualizaci√≥n

**Services:**
- `grafana` (ClusterIP) ‚Äî Acceso interno al UI

**Ingress:**
- `grafana` (ALB) ‚Äî Acceso p√∫blico al UI

**Integraci√≥n AWS:**
1. Alloy asume rol `GrafanaAlloyRole-eks-observability-v2` v√≠a IRSA
2. Firma requests HTTP a AMP con SigV4 (credenciales temporales)
3. Env√≠a m√©tricas cada 15s a remote_write endpoint
4. AMP almacena m√©tricas con retenci√≥n de 150 d√≠as
5. Grafana consulta AMP usando SigV4 (autenticaci√≥n autom√°tica con AWS SDK)

### Acceso a Grafana

**URL:** http://k8s-observab-grafana-536238b7ca-1103291513.us-east-1.elb.amazonaws.com

**Credenciales:**
- Usuario: `admin`
- Password: `admin123`

**Datasource Pre-configurado:**
- Nombre: `Amazon Managed Prometheus`
- Tipo: `Prometheus`
- URL: `https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-1c2bc642-d761-4c63-a58e-e12da54d36f1`
- Autenticaci√≥n: SigV4 (regi√≥n: us-east-1)
- Estado: Activo


## HITO 4: Aplicaci√≥n Demo & Networking (ALB + Ingress)

### Objetivo
Desplegar aplicaci√≥n Hello World (Nginx) como Deployment stateless, exponerla v√≠a AWS Load Balancer (ALB) usando Ingress, y asegurar health checks y escalado.

### Integraci√≥n con AWS
- **ALB (Application Load Balancer):** AWS Load Balancer Controller convierte recursos Ingress en ALB en AWS.
  - Listener en puerto 80/443.
  - Target Group con pods de `hello-world` (IP targets).
  - Health checks: HTTP GET `/` puerto 80.
- **IAM Role (IRSA):** Controller requiere rol con permisos `elbv2:*`, `ec2:Describe*`, `iam:PassRole`.
- **Security Groups:** Abiertos entre ALB (public subnet) y nodos (private subnet).

### Integraci√≥n con EKS
- **Namespace:** `app-demo`.
- **ServiceAccount:** `app-demo-sa` anotado con rol IRSA (si consume secretos de AWS).
- **Deployment:** `hello-world` (1-3 replicas, escalable).
  - Readiness probe: HTTP GET `/` con delay 5s.
  - Liveness probe: HTTP GET `/health` con delay 10s.
  - Requests: CPU 100m, Memory 128Mi.
  - Limits: CPU 250m, Memory 256Mi.
- **Service:** `hello-world` (ClusterIP) para comunicaci√≥n intra-cluster.
- **Ingress:** `hello-world-ingress` anotado con ALB controller settings.
- **AWS Load Balancer Controller:** Deployment en `kube-system` (1 replica) que gestiona Ingress ‚Üî ALB.

### Despliegue con Terraform
- **M√≥dulo:** `terraform/modules/alb_controller` y `terraform/modules/app_demo`.
- **Ubicaci√≥n:** `terraform/main.tf` o `terraform/app-demo.tf`.
- **Recursos clave:**
  - `aws_iam_role` ‚Äî Rol para ALB Controller (IRSA).
  - `aws_iam_role_policy` ‚Äî Permisos ELBv2 y EC2.
  - `helm_release` ‚Äî Chart AWS Load Balancer Controller (repo `https://aws.github.io/eks-charts`).
  - `kubernetes_namespace` ‚Äî Namespace `app-demo`.
  - `kubernetes_deployment` ‚Äî Hello World Deployment.
  - `kubernetes_service` ‚Äî Service hello-world.
  - `kubernetes_ingress` ‚Äî Ingress hello-world-ingress.

**Ingress manifiesto (en Terraform o `examples/ingress-hello-world.yaml`):**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-world-ingress
  namespace: app-demo
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: "30"
spec:
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: hello-world
            port:
              number: 80
```

**Hello World Deployment (en Terraform o `examples/deployment-hello-world.yaml`):**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-world
  namespace: app-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hello-world
  template:
    metadata:
      labels:
        app: hello-world
    spec:
      serviceAccountName: app-demo-sa
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 20
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "250m"
            memory: "256Mi"
        env:
        - name: APP_NAME
          value: "hello-world"
```

### Componentes en el Cluster (resultado del Hito 4)
**Namespace:** `app-demo`.

**Pods:**
- `hello-world` Deployment (2-3 replicas, escalable con HPA) ‚Äî Aplicaci√≥n Nginx.
- `aws-load-balancer-controller` Deployment en `kube-system` (1 replica) ‚Äî Controller de ALB.

**Kubernetes Resources:**
- Service `hello-world` (ClusterIP).
- Ingress `hello-world-ingress` (convierte en ALB).

**AWS Resources (auto-creados por ALB Controller):**
- ALB en subnets p√∫blicas.
- Target Group con pods de hello-world.
- Listener HTTP 80 ‚Üí Target Group.

**Integraci√≥n:**
- Cliente ‚Üí ALB (public subnet) ‚Üí Ingress Controller ‚Üí hello-world pods (private subnet).
- Alloy scrapea m√©tricas de hello-world en namespace `app-demo`.

**Total de pods app-demo: 2-3 (hello-world replicas).**

---

## HITO 5: Gesti√≥n de Secretos (ESO + Secrets Manager)

### Objetivo
Sincronizar secretos desde AWS Secrets Manager al cluster usando External Secrets Operator (ESO) o Secrets Store CSI Driver, sin exponer credenciales en c√≥digo.

### Integraci√≥n con AWS
- **AWS Secrets Manager:** Almacena secretos (API keys, passwords, DB credentials).
  - Secreto ejemplo: `eks-observability-app-secret` con propiedades (API_KEY, DB_PASSWORD, etc.).
- **IAM Role (IRSA):** Rol ESO con permiso `secretsmanager:GetSecretValue` y `kms:Decrypt` (si usa KMS).
- **KMS (opcional):** Cifrado de secretos en reposo.
- **Audit (CloudTrail):** Registra acceso a secretos para compliance.

### Integraci√≥n con EKS
- **Namespace:** `external-secrets-system` (o `app-demo` si se coloca ESO ah√≠).
- **ServiceAccount:** `external-secrets` anotado con rol IRSA.
- **Controllers:**
  - `external-secrets-controller` (Deployment, 1 replica) ‚Äî Sincroniza secretos.
  - `external-secrets-webhook` (Deployment, 1 replica) ‚Äî Valida manifiestos.
- **CRDs:**
  - `SecretStore` ‚Äî Configuraci√≥n de provider AWS Secrets Manager.
  - `ExternalSecret` ‚Äî Declara qu√© secreto sincronizar desde AWS y a d√≥nde en K8s.
- **Resultado:** Kubernetes Secrets creados din√°micamente y sincronizados cada 15-30s.

### Despliegue con Terraform
- **M√≥dulo:** `terraform/modules/external_secrets`.
- **Ubicaci√≥n:** `terraform/main.tf` o `terraform/external-secrets.tf`.
- **Recursos clave:**
  - `aws_iam_role` ‚Äî Rol ESO para IRSA.
  - `aws_iam_role_policy` ‚Äî Permisos Secrets Manager y KMS.
  - `helm_release` ‚Äî Chart External Secrets (repo `https://external-secrets.github.io/kubernetes-external-secrets/`).
  - `kubernetes_namespace` ‚Äî Namespace `external-secrets-system`.
  - `kubernetes_manifest` ‚Äî SecretStore y ExternalSecret (si se gestionan en K8s).

**SecretStore manifiesto (en `examples/secretstore-externalsecret.yaml` o Terraform):**
```yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secretsmanager
  namespace: app-demo
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-east-1
      auth:
        jwt:
          serviceAccountRef:
            name: external-secrets
```

**ExternalSecret manifiesto (en `examples/`):**
```yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: app-secret-external
  namespace: app-demo
spec:
  refreshInterval: 15s
  secretStoreRef:
    name: aws-secretsmanager
    kind: SecretStore
  target:
    name: app-secret
    creationPolicy: Owner
  data:
  - secretKey: API_KEY
    remoteRef:
      key: eks-observability-app-secret
      property: API_KEY
  - secretKey: DB_PASSWORD
    remoteRef:
      key: eks-observability-app-secret
      property: DB_PASSWORD
```

### Componentes en el Cluster (resultado del Hito 5)
**Namespace:** `external-secrets-system`.

**Pods:**
- `external-secrets-controller` Deployment (1 replica) ‚Äî Sincronizador de secretos.
- `external-secrets-webhook` Deployment (1 replica) ‚Äî Validador.

**Kubernetes Resources:**
- `SecretStore` en `app-demo` ‚Äî Configuraci√≥n de proveedor AWS.
- `ExternalSecret` en `app-demo` ‚Äî Declaraci√≥n de qu√© sincronizar.
- `Secret` en `app-demo` (auto-creado) ‚Äî Secreto sincronizado desde AWS.

**Integraci√≥n AWS:**
- ESO controller asume rol IAM v√≠a IRSA.
- Cada 15s, ESO llama a `secretsmanager:GetSecretValue` para obtener valor actual.
- Si cambia en Secrets Manager, K8s Secret se actualiza autom√°ticamente.

**Consumo en App:**
- Hello World Deployment monta Secret como variables de entorno o vol√∫menes.

**Total de pods ESO: 2 (controller + webhook).**

---

---

## üìä Resumen de Componentes Totales en el Cluster

### Namespaces (9):
1. `kube-system`
2. `kube-public`
3. `default`
4. `observability`
5. `karpenter`
6. `kube-node-lease`
7. `app-demo`
8. `external-secrets-system`
9. `istio-system` (opcional)


### Validaci√≥n post-despliegue:
```bash
# Verificar namespaces
kubectl get namespaces

# Verificar pods en cada namespace
kubectl get pods --all-namespaces

# Verificar nodos
kubectl get nodes

# Verificar servicios y Ingress
kubectl get svc,ingress -A

# Verificar que Alloy est√° enviando a AMP
kubectl logs -n observability daemonset/grafana-alloy -c alloy --tail=50
```

---

## üìù Estructura de Carpetas Terraform Recomendada

```
terraform/
‚îú‚îÄ‚îÄ main.tf                 # Hito 1 (VPC, EKS, IAM base)
‚îú‚îÄ‚îÄ karpenter.tf           # Hito 2 (Karpenter, Provisioner)
‚îú‚îÄ‚îÄ observability.tf       # Hito 3 (Alloy, AMP, kube-state-metrics)
‚îú‚îÄ‚îÄ alb-app-demo.tf       # Hito 4 (ALB Controller, Hello World)
‚îú‚îÄ‚îÄ external-secrets.tf    # Hito 5 (ESO, SecretStore, ExternalSecret)
‚îú‚îÄ‚îÄ providers.tf           # Providers (AWS, Kubernetes, Helm)
‚îú‚îÄ‚îÄ variables.tf           # Variables (cluster name, region, etc.)
‚îú‚îÄ‚îÄ outputs.tf             # Outputs (ALB DNS, AMP workspace ID, etc.)
‚îú‚îÄ‚îÄ terraform.tfvars       # Valores espec√≠ficos por entorno
‚îî‚îÄ‚îÄ modules/
    ‚îú‚îÄ‚îÄ network/          # VPC, subnets, NAT, IGW
    ‚îú‚îÄ‚îÄ eks/              # EKS cluster, control plane
    ‚îú‚îÄ‚îÄ karpenter/        # Karpenter role, policy, Provisioner
    ‚îú‚îÄ‚îÄ observability/    # Alloy, AMP, kube-state-metrics
    ‚îú‚îÄ‚îÄ alb_controller/   # ALB Controller role, policy, helm
    ‚îú‚îÄ‚îÄ app_demo/         # Hello World deployment, service, ingress
    ‚îî‚îÄ‚îÄ external_secrets/ # ESO role, policy, helm, SecretStore
```

